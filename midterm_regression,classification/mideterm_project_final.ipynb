{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mideterm_project_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Xyd9bb1_24in"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mong-head/machine_learning/blob/master/mideterm_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-5rKiJSFWCc",
        "colab_type": "text"
      },
      "source": [
        "# drive 저장 및 set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBba8RfrFUyi",
        "colab_type": "code",
        "outputId": "7315a1dd-70e7-4ec5-8c61-3efdc1be8f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYqmNGRLKzP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "from pandas import Series, DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3zYmVIvFiMd",
        "colab_type": "text"
      },
      "source": [
        "# * data 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh-ummzBFq3q",
        "colab_type": "text"
      },
      "source": [
        "Load TADPOLE* dataset (csv file) from Google Drive\n",
        "-------------------------------------------------------\n",
        "*The Alzheimer's Disease Prediction Of Longitudinal Evolution\n",
        "(https://tadpole.grand-challenge.org/)\n",
        "\n",
        "### -Subjects: 1707 (1363 Train (80%) + 344 Test (20%))\n",
        "### -Features: 72\n",
        "*   2 demographic feature: MMSE, ADAS13\n",
        "*   70 mean values of cortical thickness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1HB0gX2F5SU",
        "colab_type": "text"
      },
      "source": [
        "데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Xr3SiaFoPT",
        "colab_type": "code",
        "outputId": "ecb1d494-e328-4f3e-c43d-8d8076493bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "csv_file_train = '/content/gdrive/My Drive/BNCS401_Midterm_Project/Train_data_reupdated.csv'  # Set your path\n",
        "train_data = pd.read_csv(csv_file_train)\n",
        "train_data\n",
        "\n",
        "# DXCHANGE: clinical label (1-CN, 2-MCI, 3-AD)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RID</th>\n",
              "      <th>DXCHANGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ADAS13</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4084</td>\n",
              "      <td>1</td>\n",
              "      <td>68.4</td>\n",
              "      <td>30</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.613</td>\n",
              "      <td>2.904</td>\n",
              "      <td>2.311</td>\n",
              "      <td>1.647</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.604</td>\n",
              "      <td>2.480</td>\n",
              "      <td>3.095</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.207</td>\n",
              "      <td>2.903</td>\n",
              "      <td>2.617</td>\n",
              "      <td>4.117</td>\n",
              "      <td>2.701</td>\n",
              "      <td></td>\n",
              "      <td>3.127</td>\n",
              "      <td>3.051</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.026</td>\n",
              "      <td>3.756</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.762</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.695</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.690</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.421</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.370</td>\n",
              "      <td>2.674</td>\n",
              "      <td>3.004</td>\n",
              "      <td>2.369</td>\n",
              "      <td>1.599</td>\n",
              "      <td>2.208</td>\n",
              "      <td>2.650</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.544</td>\n",
              "      <td>3.018</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.880</td>\n",
              "      <td>2.322</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.489</td>\n",
              "      <td>3.620</td>\n",
              "      <td>2.711</td>\n",
              "      <td></td>\n",
              "      <td>2.593</td>\n",
              "      <td>2.792</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.993</td>\n",
              "      <td>3.734</td>\n",
              "      <td>2.390</td>\n",
              "      <td>2.817</td>\n",
              "      <td>2.471</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.490</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.171</td>\n",
              "      <td>2.862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2196</td>\n",
              "      <td>2</td>\n",
              "      <td>68.2</td>\n",
              "      <td>30</td>\n",
              "      <td>13.00</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.992</td>\n",
              "      <td>2.470</td>\n",
              "      <td>2.965</td>\n",
              "      <td>2.438</td>\n",
              "      <td>1.584</td>\n",
              "      <td>1.910</td>\n",
              "      <td>2.900</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.771</td>\n",
              "      <td>2.354</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.363</td>\n",
              "      <td>3.613</td>\n",
              "      <td>2.475</td>\n",
              "      <td></td>\n",
              "      <td>3.196</td>\n",
              "      <td>3.334</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.627</td>\n",
              "      <td>1.742</td>\n",
              "      <td>3.383</td>\n",
              "      <td>2.647</td>\n",
              "      <td>2.758</td>\n",
              "      <td>2.394</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.241</td>\n",
              "      <td>2.824</td>\n",
              "      <td>1.865</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.866</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.874</td>\n",
              "      <td>2.316</td>\n",
              "      <td>1.478</td>\n",
              "      <td>1.909</td>\n",
              "      <td>2.780</td>\n",
              "      <td>2.589</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.036</td>\n",
              "      <td>2.329</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.070</td>\n",
              "      <td>2.783</td>\n",
              "      <td>2.594</td>\n",
              "      <td>3.405</td>\n",
              "      <td>2.367</td>\n",
              "      <td></td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.977</td>\n",
              "      <td>2.489</td>\n",
              "      <td>1.868</td>\n",
              "      <td>3.220</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.372</td>\n",
              "      <td>2.854</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.233</td>\n",
              "      <td>2.793</td>\n",
              "      <td>1.987</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>657</td>\n",
              "      <td>1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>29</td>\n",
              "      <td>15.33</td>\n",
              "      <td>2.249</td>\n",
              "      <td>2.296</td>\n",
              "      <td>2.315</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.327</td>\n",
              "      <td>2.193</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.559</td>\n",
              "      <td>2.008</td>\n",
              "      <td>2.495</td>\n",
              "      <td>2.418</td>\n",
              "      <td>4.210</td>\n",
              "      <td>2.167</td>\n",
              "      <td>1.08</td>\n",
              "      <td>3.362</td>\n",
              "      <td>3.077</td>\n",
              "      <td>2.648</td>\n",
              "      <td>2.759</td>\n",
              "      <td>2.442</td>\n",
              "      <td>1.717</td>\n",
              "      <td>2.886</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.737</td>\n",
              "      <td>2.402</td>\n",
              "      <td>3.012</td>\n",
              "      <td>2.418</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.805</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.318</td>\n",
              "      <td>3.025</td>\n",
              "      <td>2.270</td>\n",
              "      <td>1.831</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.335</td>\n",
              "      <td>1.389</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.397</td>\n",
              "      <td>2.187</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.889</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.028</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.309</td>\n",
              "      <td>3.880</td>\n",
              "      <td>1.832</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.343</td>\n",
              "      <td>2.363</td>\n",
              "      <td>1.601</td>\n",
              "      <td>3.683</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.415</td>\n",
              "      <td>2.252</td>\n",
              "      <td>2.583</td>\n",
              "      <td>1.850</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4526</td>\n",
              "      <td>3</td>\n",
              "      <td>79.4</td>\n",
              "      <td>22</td>\n",
              "      <td>24.00</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.289</td>\n",
              "      <td>2.258</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.124</td>\n",
              "      <td>1.473</td>\n",
              "      <td>1.729</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.184</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.680</td>\n",
              "      <td>2.218</td>\n",
              "      <td>2.407</td>\n",
              "      <td>1.923</td>\n",
              "      <td>2.542</td>\n",
              "      <td>2.296</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.157</td>\n",
              "      <td></td>\n",
              "      <td>2.671</td>\n",
              "      <td>2.925</td>\n",
              "      <td>2.288</td>\n",
              "      <td>3.322</td>\n",
              "      <td>2.336</td>\n",
              "      <td>1.691</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.940</td>\n",
              "      <td>2.178</td>\n",
              "      <td>2.079</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.083</td>\n",
              "      <td>1.904</td>\n",
              "      <td>2.400</td>\n",
              "      <td>1.750</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.493</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.399</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.073</td>\n",
              "      <td>1.506</td>\n",
              "      <td>1.777</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.087</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.410</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.269</td>\n",
              "      <td>3.085</td>\n",
              "      <td>2.080</td>\n",
              "      <td></td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.372</td>\n",
              "      <td>1.757</td>\n",
              "      <td>2.651</td>\n",
              "      <td>2.665</td>\n",
              "      <td>2.578</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.537</td>\n",
              "      <td>1.958</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>362</td>\n",
              "      <td>2</td>\n",
              "      <td>70.5</td>\n",
              "      <td>24</td>\n",
              "      <td>20.33</td>\n",
              "      <td>1.765</td>\n",
              "      <td>2.081</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.140</td>\n",
              "      <td>1.466</td>\n",
              "      <td>1.749</td>\n",
              "      <td>2.025</td>\n",
              "      <td>2.021</td>\n",
              "      <td>1.938</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.114</td>\n",
              "      <td>2.368</td>\n",
              "      <td>1.811</td>\n",
              "      <td>2.227</td>\n",
              "      <td>2.191</td>\n",
              "      <td>3.188</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.013</td>\n",
              "      <td>2.548</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.280</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.291</td>\n",
              "      <td>1.821</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.248</td>\n",
              "      <td>2.414</td>\n",
              "      <td>2.164</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.012</td>\n",
              "      <td>1.916</td>\n",
              "      <td>2.451</td>\n",
              "      <td>1.959</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.405</td>\n",
              "      <td>1.968</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.201</td>\n",
              "      <td>2.711</td>\n",
              "      <td>2.287</td>\n",
              "      <td>1.753</td>\n",
              "      <td>1.644</td>\n",
              "      <td>2.209</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.745</td>\n",
              "      <td>2.102</td>\n",
              "      <td>2.519</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.191</td>\n",
              "      <td>0.922</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.273</td>\n",
              "      <td>1.662</td>\n",
              "      <td>2.790</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.348</td>\n",
              "      <td>2.197</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.844</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.121</td>\n",
              "      <td>1.951</td>\n",
              "      <td>2.262</td>\n",
              "      <td>2.245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>4187</td>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>29</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.441</td>\n",
              "      <td>2.813</td>\n",
              "      <td>2.614</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.354</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.395</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.005</td>\n",
              "      <td>2.571</td>\n",
              "      <td>2.312</td>\n",
              "      <td>2.923</td>\n",
              "      <td>2.596</td>\n",
              "      <td>3.972</td>\n",
              "      <td>2.793</td>\n",
              "      <td></td>\n",
              "      <td>3.396</td>\n",
              "      <td>3.230</td>\n",
              "      <td>2.417</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.544</td>\n",
              "      <td>1.938</td>\n",
              "      <td>3.513</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.371</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.434</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.717</td>\n",
              "      <td>2.096</td>\n",
              "      <td>2.335</td>\n",
              "      <td>2.996</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.426</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.476</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.357</td>\n",
              "      <td>3.116</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.270</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.533</td>\n",
              "      <td>3.715</td>\n",
              "      <td>2.815</td>\n",
              "      <td></td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.035</td>\n",
              "      <td>3.865</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.850</td>\n",
              "      <td>2.524</td>\n",
              "      <td>3.213</td>\n",
              "      <td>2.489</td>\n",
              "      <td>2.455</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.209</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>4928</td>\n",
              "      <td>2</td>\n",
              "      <td>77.8</td>\n",
              "      <td>27</td>\n",
              "      <td>17.00</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.392</td>\n",
              "      <td>2.158</td>\n",
              "      <td>1.686</td>\n",
              "      <td>1.914</td>\n",
              "      <td>2.318</td>\n",
              "      <td>2.297</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.908</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.067</td>\n",
              "      <td>2.385</td>\n",
              "      <td>2.458</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.360</td>\n",
              "      <td></td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.919</td>\n",
              "      <td>2.063</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.284</td>\n",
              "      <td>1.906</td>\n",
              "      <td>3.070</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.235</td>\n",
              "      <td>2.685</td>\n",
              "      <td>2.419</td>\n",
              "      <td>1.953</td>\n",
              "      <td>2.660</td>\n",
              "      <td>1.815</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.681</td>\n",
              "      <td>2.349</td>\n",
              "      <td>2.211</td>\n",
              "      <td>2.330</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.196</td>\n",
              "      <td>1.565</td>\n",
              "      <td>1.845</td>\n",
              "      <td>2.281</td>\n",
              "      <td>2.255</td>\n",
              "      <td>2.266</td>\n",
              "      <td>2.893</td>\n",
              "      <td>2.167</td>\n",
              "      <td>2.420</td>\n",
              "      <td>1.934</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.420</td>\n",
              "      <td>3.288</td>\n",
              "      <td>2.411</td>\n",
              "      <td></td>\n",
              "      <td>2.147</td>\n",
              "      <td>2.478</td>\n",
              "      <td>2.319</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.106</td>\n",
              "      <td>2.742</td>\n",
              "      <td>2.369</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.485</td>\n",
              "      <td>2.162</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.375</td>\n",
              "      <td>1.917</td>\n",
              "      <td>2.199</td>\n",
              "      <td>2.709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>4887</td>\n",
              "      <td>3</td>\n",
              "      <td>73.8</td>\n",
              "      <td>26</td>\n",
              "      <td>27.00</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.196</td>\n",
              "      <td>2.263</td>\n",
              "      <td>2.099</td>\n",
              "      <td>1.343</td>\n",
              "      <td>1.858</td>\n",
              "      <td>2.676</td>\n",
              "      <td>2.135</td>\n",
              "      <td>1.955</td>\n",
              "      <td>2.882</td>\n",
              "      <td>2.042</td>\n",
              "      <td>2.448</td>\n",
              "      <td>1.874</td>\n",
              "      <td>2.486</td>\n",
              "      <td>2.061</td>\n",
              "      <td>3.193</td>\n",
              "      <td>1.800</td>\n",
              "      <td></td>\n",
              "      <td>3.054</td>\n",
              "      <td>2.921</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.629</td>\n",
              "      <td>2.119</td>\n",
              "      <td>1.730</td>\n",
              "      <td>3.190</td>\n",
              "      <td>2.839</td>\n",
              "      <td>2.443</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.567</td>\n",
              "      <td>1.854</td>\n",
              "      <td>2.713</td>\n",
              "      <td>1.646</td>\n",
              "      <td>2.698</td>\n",
              "      <td>2.424</td>\n",
              "      <td>1.969</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.231</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.340</td>\n",
              "      <td>1.472</td>\n",
              "      <td>1.775</td>\n",
              "      <td>2.832</td>\n",
              "      <td>2.014</td>\n",
              "      <td>2.200</td>\n",
              "      <td>2.720</td>\n",
              "      <td>2.320</td>\n",
              "      <td>2.427</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.362</td>\n",
              "      <td>2.122</td>\n",
              "      <td>3.244</td>\n",
              "      <td>2.270</td>\n",
              "      <td></td>\n",
              "      <td>2.577</td>\n",
              "      <td>2.239</td>\n",
              "      <td>2.084</td>\n",
              "      <td>1.682</td>\n",
              "      <td>2.797</td>\n",
              "      <td>3.351</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.683</td>\n",
              "      <td>1.701</td>\n",
              "      <td>2.749</td>\n",
              "      <td>2.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1205</td>\n",
              "      <td>3</td>\n",
              "      <td>83.0</td>\n",
              "      <td>23</td>\n",
              "      <td>29.33</td>\n",
              "      <td>1.822</td>\n",
              "      <td>2.508</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.000</td>\n",
              "      <td>1.374</td>\n",
              "      <td>1.465</td>\n",
              "      <td>2.589</td>\n",
              "      <td>1.677</td>\n",
              "      <td>1.932</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.215</td>\n",
              "      <td>1.617</td>\n",
              "      <td>2.066</td>\n",
              "      <td>1.910</td>\n",
              "      <td>3.069</td>\n",
              "      <td>1.536</td>\n",
              "      <td>1.043</td>\n",
              "      <td>2.902</td>\n",
              "      <td>2.809</td>\n",
              "      <td>1.875</td>\n",
              "      <td>2.716</td>\n",
              "      <td>2.001</td>\n",
              "      <td>1.396</td>\n",
              "      <td>2.681</td>\n",
              "      <td>1.613</td>\n",
              "      <td>2.239</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.561</td>\n",
              "      <td>2.520</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.414</td>\n",
              "      <td>1.736</td>\n",
              "      <td>1.962</td>\n",
              "      <td>2.296</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.468</td>\n",
              "      <td>1.769</td>\n",
              "      <td>2.079</td>\n",
              "      <td>1.926</td>\n",
              "      <td>1.232</td>\n",
              "      <td>1.501</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.831</td>\n",
              "      <td>1.939</td>\n",
              "      <td>3.250</td>\n",
              "      <td>1.900</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.193</td>\n",
              "      <td>1.968</td>\n",
              "      <td>3.351</td>\n",
              "      <td>1.494</td>\n",
              "      <td>0.997</td>\n",
              "      <td>1.828</td>\n",
              "      <td>3.025</td>\n",
              "      <td>1.852</td>\n",
              "      <td>1.463</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.408</td>\n",
              "      <td>1.849</td>\n",
              "      <td>2.449</td>\n",
              "      <td>2.574</td>\n",
              "      <td>1.784</td>\n",
              "      <td>2.515</td>\n",
              "      <td>1.806</td>\n",
              "      <td>2.071</td>\n",
              "      <td>2.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>2099</td>\n",
              "      <td>2</td>\n",
              "      <td>83.7</td>\n",
              "      <td>30</td>\n",
              "      <td>12.00</td>\n",
              "      <td>2.245</td>\n",
              "      <td>2.351</td>\n",
              "      <td>2.305</td>\n",
              "      <td>2.475</td>\n",
              "      <td>2.155</td>\n",
              "      <td>1.676</td>\n",
              "      <td>1.857</td>\n",
              "      <td>2.525</td>\n",
              "      <td>2.085</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.161</td>\n",
              "      <td>2.295</td>\n",
              "      <td>1.948</td>\n",
              "      <td>2.424</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.958</td>\n",
              "      <td>1.778</td>\n",
              "      <td></td>\n",
              "      <td>2.672</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.693</td>\n",
              "      <td>2.315</td>\n",
              "      <td>1.637</td>\n",
              "      <td>3.445</td>\n",
              "      <td>2.396</td>\n",
              "      <td>2.432</td>\n",
              "      <td>2.265</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.021</td>\n",
              "      <td>2.350</td>\n",
              "      <td>1.699</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.625</td>\n",
              "      <td>2.034</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.237</td>\n",
              "      <td>2.236</td>\n",
              "      <td>2.314</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.868</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.072</td>\n",
              "      <td>2.060</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.155</td>\n",
              "      <td>2.376</td>\n",
              "      <td>1.894</td>\n",
              "      <td>2.400</td>\n",
              "      <td>2.330</td>\n",
              "      <td>3.177</td>\n",
              "      <td>2.054</td>\n",
              "      <td></td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.206</td>\n",
              "      <td>1.785</td>\n",
              "      <td>3.093</td>\n",
              "      <td>2.310</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.381</td>\n",
              "      <td>1.817</td>\n",
              "      <td>2.121</td>\n",
              "      <td>2.586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1363 rows × 75 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       RID  ...  ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
              "0     4084  ...                                       2.862\n",
              "1     2196  ...                                       2.943\n",
              "2      657  ...                                       2.828\n",
              "3     4526  ...                                       2.591\n",
              "4      362  ...                                       2.245\n",
              "...    ...  ...                                         ...\n",
              "1358  4187  ...                                       2.935\n",
              "1359  4928  ...                                       2.709\n",
              "1360  4887  ...                                       2.777\n",
              "1361  1205  ...                                       2.248\n",
              "1362  2099  ...                                       2.586\n",
              "\n",
              "[1363 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9YXxbthGCEV",
        "colab_type": "text"
      },
      "source": [
        "train_data.info()로 확인하니, 빠져있는 데이터도 있고(예측값인 ADAS13, cortical값 중에 1개), 실제로는 NaN값이지만 ' '로 채워진 데이터가 2개가 있었다.\n",
        "\n",
        "' '로 채워져 있어 object인 두 cortical값\n",
        "*   ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
        "*   ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
        "\n",
        "데이터 처리 순서\n",
        "1. object 처리 : space to NaN\n",
        "2. imputer : median으로 빠져있는 값들 채우기\n",
        "3. feature scaling - RobustScaler : cortical, age 값의 범위를 같게 맞춘다.\n",
        "\n",
        "** StandardScaler안쓰고 RobustScaler하는 이유 : 실제로 이게 더 나았음- outlier제거가 되기때문임\n",
        "\n",
        "2,3을 pipeline으로 하고, 2 전에 AD_tr(cortical값들, age),y(reg/cls)로 나누고,\n",
        "\n",
        "AD_tr만 pipeline해서 X로 만들고, y는 imputer만 한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxeqPNHiLa6B",
        "colab_type": "text"
      },
      "source": [
        "1) object처리\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd3Vp4OKLrEU",
        "colab_type": "code",
        "outputId": "f24acfbb-dc22-4bab-8e48-2232d1eec61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#2개의 cortical feature가 float이 아니라 object이고, ' '로 채워져있다.\n",
        "ob1 = train_data[\"ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\"]\n",
        "ob2 = train_data[\"ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\"]\n",
        "print(len(train_data[ob1.isnull()]), ob1.dtypes)\n",
        "print(len(train_data[ob2.isnull()]),ob2.dtypes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 object\n",
            "0 object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGK2gPstLUv1",
        "colab_type": "code",
        "outputId": "e3a45349-f7a6-4035-f2de-3dd7f8af7cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#그냥 ' '로 뜬다.\n",
        "train_data[\"ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\"][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2YObSwZF6-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def object_to_float(o): #o = train_data[\"~~\"] 형태\n",
        "  for i in range(1363):\n",
        "    if o[i] == ' ':\n",
        "      o[i] = np.nan #nan으로 채움"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07BKTvJ9Li1A",
        "colab_type": "code",
        "outputId": "bb6c9ea6-4e57-4fcf-f73b-e3d9ca5da0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "object_to_float(ob1)\n",
        "object_to_float(ob2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOoWsxVL5OF",
        "colab_type": "code",
        "outputId": "0ddfb18d-3d3d-4a35-c274-cc906f61ffab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(train_data[ob1.isnull()]), ob1.dtypes)\n",
        "print(len(train_data[ob2.isnull()]),ob2.dtypes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "724 object\n",
            "724 object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Leqdxx1PMAFu",
        "colab_type": "text"
      },
      "source": [
        "**train, test set나누기\n",
        "---\n",
        "AD_train, AD_test = "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r74XRsriOckn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RID는 버림\n",
        "train_data = train_data.drop(\"RID\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cK2I-QAbLOM",
        "colab_type": "text"
      },
      "source": [
        "** trian, test set 나누기\n",
        "---\n",
        "train set에서 model들의 train loss와 validation loss등을 보며 model을 결정하고\n",
        "\n",
        "test set에서 마지막으로 검사해보았을 때에도 최적의 모델이라면 \n",
        "\n",
        "그 모델을 선택하고, 마지막에는 train, test set 나누기 전의 X로 최종으로 선택했던 모델로 마지막 training시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh0HjDkEL_FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "AD_train, AD_test = train_test_split(train_data,test_size=0.2,random_state= 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xKAz8T8R1Hc",
        "colab_type": "text"
      },
      "source": [
        "** AD_tr, y나누고, y에는 imputer\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LWik7raS1LC",
        "colab_type": "text"
      },
      "source": [
        "RID는 버린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T592RMMTPH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rksVotP_R5oU",
        "colab_type": "code",
        "outputId": "85911d29-3d46-4a6b-ddbc-0bebdb0cc05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "##input 해당\n",
        "X_prepipeline = train_data.drop(\"MMSE\",axis = 1).drop(\"ADAS13\",axis = 1).drop(\"DXCHANGE\",axis = 1)\n",
        "X_train_prepipeline = AD_train.drop(\"MMSE\",axis = 1).drop(\"ADAS13\",axis = 1).drop(\"DXCHANGE\",axis = 1)\n",
        "X_test_prepipeline = AD_test.drop(\"MMSE\",axis = 1).drop(\"ADAS13\",axis = 1).drop(\"DXCHANGE\",axis = 1)\n",
        "\n",
        "#classification input\n",
        "X_plus_prepipeline = train_data.drop(\"DXCHANGE\",axis = 1)\n",
        "X_plus_train_prepipeline = AD_train.drop(\"DXCHANGE\",axis = 1)\n",
        "X_plus_test_prepipeline = AD_test.drop(\"DXCHANGE\",axis = 1)\n",
        "\n",
        "##output 해당\n",
        "AD_label_reg = DataFrame({\"MMSE\":train_data[\"MMSE\"].copy(),\n",
        "                         \"ADAS13\":train_data[\"ADAS13\"].copy()})\n",
        "AD_label_reg_tr = DataFrame({\"MMSE\":AD_train[\"MMSE\"].copy(),\n",
        "                         \"ADAS13\":AD_train[\"ADAS13\"].copy()})\n",
        "AD_label_reg_test = DataFrame({\"MMSE\":AD_test[\"MMSE\"].copy(),\n",
        "                         \"ADAS13\":AD_test[\"ADAS13\"].copy()})\n",
        "\n",
        "imputer.fit(AD_label_reg)\n",
        "temp = imputer.transform(AD_label_reg)\n",
        "y_reg = pd.DataFrame(temp, columns=AD_label_reg.columns,\n",
        "                             index=AD_label_reg.index)\n",
        "\n",
        "imputer.fit(AD_label_reg_tr)\n",
        "temp = imputer.transform(AD_label_reg_tr)\n",
        "y_reg_train = pd.DataFrame(temp, columns=AD_label_reg_tr.columns,\n",
        "                             index=AD_label_reg_tr.index)\n",
        "imputer.fit(AD_label_reg_test)\n",
        "temp = imputer.transform(AD_label_reg_test)\n",
        "y_reg_test = pd.DataFrame(temp, columns=AD_label_reg_test.columns,\n",
        "                             index=AD_label_reg_test.index)\n",
        "\n",
        "y_mmse =  DataFrame({\"MMSE\":y_reg[\"MMSE\"].copy()})\n",
        "y_mmse_train = DataFrame({\"MMSE\":y_reg_train[\"MMSE\"].copy()})\n",
        "y_mmse_test = DataFrame({\"MMSE\":y_reg_test[\"MMSE\"].copy()})\n",
        "\n",
        "y_ada =  DataFrame({\"ADAS13\":y_reg[\"ADAS13\"].copy()})\n",
        "y_ada_train = DataFrame({\"ADAS13\":y_reg_train[\"ADAS13\"].copy()})\n",
        "y_ada_test = DataFrame({\"ADAS13\":y_reg_test[\"ADAS13\"].copy()})\n",
        "\n",
        "y_cls =  DataFrame({\"DXCHANGE\":train_data[\"DXCHANGE\"]})\n",
        "y_cls_train = DataFrame({\"DXCHANGE\":AD_train[\"DXCHANGE\"]})\n",
        "y_cls_test = DataFrame({\"DXCHANGE\":AD_test[\"DXCHANGE\"]})\n",
        "\n",
        "incomplete_rows_before = AD_label_reg_tr[AD_label_reg_tr.isnull().any(axis=1)]#.head()\n",
        "incomplete_rows_after = y_reg_train[y_reg_train.isnull().any(axis=1)]\n",
        "print(\"1. before imputer:\\n\", incomplete_rows_before)\n",
        "print(\"\\n2. after imputer : \\n\", incomplete_rows_after)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. before imputer:\n",
            "       MMSE  ADAS13\n",
            "522     23     NaN\n",
            "25      21     NaN\n",
            "1311    18     NaN\n",
            "999     20     NaN\n",
            "426     25     NaN\n",
            "946     29     NaN\n",
            "431     22     NaN\n",
            "\n",
            "2. after imputer : \n",
            " Empty DataFrame\n",
            "Columns: [MMSE, ADAS13]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-quVvnc0PzUw",
        "colab_type": "text"
      },
      "source": [
        "2) pipeline : imputer,RobustScaler\n",
        "---\n",
        "* imputer - 빈데이터 채우기\n",
        "* RobustScaler - 값 범위 같게 만들기, Standard보다는 outlier다루는데 더 괜찮을 거 같아서 이것을 썼다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjpD4AEZV4zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks17w2kiP4TI",
        "colab_type": "code",
        "outputId": "4ddac26a-d5cf-49a1-e682-91ea389f853c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#빈데이터 모두 보기\n",
        "incomplete_rows = X_prepipeline[X_prepipeline.isnull().any(axis=1)]#.head()\n",
        "print(\"빈데이터 개수: \",len(incomplete_rows))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "빈데이터 개수:  724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULsAzfZKVc_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_pipeline = Pipeline([\n",
        "                     ('imputer',SimpleImputer(strategy=\"median\")),\n",
        "                     ('rb_scaler',RobustScaler()),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDuq6oD7WXkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_pipeline.fit_transform(X_prepipeline)\n",
        "X_train = data_pipeline.fit_transform(X_train_prepipeline)\n",
        "X_test = data_pipeline.fit_transform(X_test_prepipeline)\n",
        "\n",
        "X_plus = data_pipeline.fit_transform(X_plus_prepipeline)\n",
        "X_plus_train = data_pipeline.fit_transform(X_plus_train_prepipeline)\n",
        "X_plus_test = data_pipeline.fit_transform(X_plus_test_prepipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lfGJNpqu24hv"
      },
      "source": [
        "# 1. Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "66CMQ0xB24hx"
      },
      "source": [
        "1) model selection\n",
        "---\n",
        "linear, poly_feature적용한 X_train_poly이용한 polynomial, decision tree, random forest비교함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xu4YBDHS24hz"
      },
      "source": [
        "train loss랑 validation 했을때 loss를 비교하고, 둘다 낮으면서 차이가 적은 것을 model로 택하는 것을 목표로 했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pS7kd5pW24h0"
      },
      "source": [
        "** model들의 train, validation loss\n",
        "---\n",
        "\n",
        "* MultiOutputRegressor를 사용해서 mmse, adas13각각에 대해 같은 모델들(linear, decision tree, randomforest)을 적용해본다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b92addab-7462-4275-920b-9691082502ec",
        "id": "ow8bFvnb24iG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#1. linear \n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_reg_train)\n",
        "lin_reg = MultiOutputRegressor(lin_reg)\n",
        "\n",
        "lin_reg.fit(X_train,y_reg_train)\n",
        "lin_predictions = lin_reg.predict(X_train)\n",
        "lin_mse = mean_squared_error(y_reg_train, lin_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "\n",
        "scores = cross_val_score(lin_reg,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "lin_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "test_lin_predictions = lin_reg.predict(X_test)\n",
        "test_lin_mse = mean_squared_error(y_reg_test, test_lin_predictions)\n",
        "test_lin_rmse = np.sqrt(test_lin_mse)\n",
        "\n",
        "#2. polynomial\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_reg_train)\n",
        "lin_reg = MultiOutputRegressor(lin_reg)\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_train = poly_features.fit_transform(X_train)\n",
        "\n",
        "lin_reg.fit(X_poly_train, y_reg_train)\n",
        "poly_lin_predictions = lin_reg.predict(X_poly_train)\n",
        "poly_lin_mse = mean_squared_error(y_reg_train, poly_lin_predictions)\n",
        "poly_lin_rmse = np.sqrt(poly_lin_mse)\n",
        "\n",
        "scores = cross_val_score(lin_reg,X_poly_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "poly_lin_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_test = poly_features.fit_transform(X_test)\n",
        "test_poly_lin_predictions = lin_reg.predict(X_poly_test)\n",
        "test_poly_lin_mse = mean_squared_error(y_reg_test, test_poly_lin_predictions)\n",
        "test_poly_lin_rmse = np.sqrt(test_poly_lin_mse)\n",
        "\n",
        "#3. decision tree\n",
        "DTR = DecisionTreeRegressor()\n",
        "DTR.fit(X_train,y_reg_train)\n",
        "DTR = MultiOutputRegressor(DTR)\n",
        "DTR.fit(X_train, y_reg_train)\n",
        "\n",
        "DTR_predictions = DTR.predict(X_train)\n",
        "DTR_mse = mean_squared_error(y_reg_train, DTR_predictions)\n",
        "DTR_rmse = np.sqrt(DTR_mse)\n",
        "\n",
        "scores = cross_val_score(DTR,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "DTR_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "test_DTR_predictions = DTR.predict(X_test)\n",
        "test_DTR_mse = mean_squared_error(y_reg_test, test_DTR_predictions)\n",
        "test_DTR_rmse = np.sqrt(test_DTR_mse)\n",
        "\n",
        "#4.\n",
        "rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
        "rf.fit(X_train,y_reg_train)\n",
        "rf = MultiOutputRegressor(rf)\n",
        "rf.fit(X_train,y_reg_train)\n",
        "\n",
        "rf_predictions = rf.predict(X_train)\n",
        "rf_mse = mean_squared_error(y_reg_train, rf_predictions)\n",
        "rf_rmse = np.sqrt(rf_mse)\n",
        "\n",
        "scores = cross_val_score(rf,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "rf_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "test_rf_predictions = rf.predict(X_test)\n",
        "test_rf_mse = mean_squared_error(y_reg_test, test_rf_predictions)\n",
        "test_rf_rmse = np.sqrt(test_rf_mse)\n",
        "\n",
        "#결과 출력\n",
        "print(\"linear train loss: \", lin_rmse)\n",
        "print(\"linear validation loss : \",lin_scores)\n",
        "print(\"linear test loss : \",test_lin_rmse)\n",
        "\n",
        "print(\"\\npoly feature linear train loss: \", poly_lin_rmse)\n",
        "print(\"poly feature linear val loss: \", poly_lin_scores)\n",
        "print(\"poly test loss : \",test_poly_lin_rmse)\n",
        "\n",
        "print(\"\\ndecisiontree train loss: \", DTR_rmse)\n",
        "print(\"decisiontree val loss: \", DTR_scores)\n",
        "print(\"decision test loss : \",test_DTR_rmse)\n",
        "\n",
        "print(\"\\nRandomForest train loss: \", rf_rmse)\n",
        "print(\"RandomForest val loss: \", rf_scores)\n",
        "print(\"randomforest test loss : \",test_rf_rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear train loss:  4.878592262781079\n",
            "linear validation loss :  5.344624551446819\n",
            "linear test loss :  5.732761024409557\n",
            "\n",
            "poly feature linear train loss:  6.557028021221182e-14\n",
            "poly feature linear val loss:  7.543699309850814\n",
            "poly test loss :  8.646282159069719\n",
            "\n",
            "decisiontree train loss:  0.0\n",
            "decisiontree val loss:  7.209927785122481\n",
            "decision test loss :  7.450108856790078\n",
            "\n",
            "RandomForest train loss:  1.9211397899018752\n",
            "RandomForest val loss:  5.222583387202576\n",
            "randomforest test loss :  5.325740797339342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84vF0IXK24iK"
      },
      "source": [
        "거의 모두 overfitting 되었지만, decision tree와 poly features가 특히 overfitting이 심하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KmLdwhcp24iL"
      },
      "source": [
        "random forest가 일단 제일 나은 것 같다. \n",
        "\n",
        "linear도 비슷하니깐 linear은 batch와 regularization을 해본 것과\n",
        "\n",
        "randomforest는 grid search로 더 나은 parameter를 찾은 estimator를 비교 할 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HhMZ0q8Y24iM"
      },
      "source": [
        "2) linear, randomforest optimization(각각)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hGoFGlBu24iN"
      },
      "source": [
        "**linear - batch GD,SGD\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2b2094aa-b697-491b-ade0-e581368b1552",
        "id": "xLmEOA0n24iO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1090"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t0ND0cZl24iQ",
        "colab": {}
      },
      "source": [
        "##batch - slow\n",
        "X_b_train = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
        "#X_b_test = np.c_[np.ones((1363*0.2,1)),X_test]\n",
        "\n",
        "eta = 0.02  # learning rate\n",
        "n_iterations = 10000\n",
        "m = X_train.shape[0]\n",
        "\n",
        "theta = np.random.randn(72,2)  # random normal initialization\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b_train.T.dot(X_b_train.dot(theta) - y_reg_train)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "prediction_lin_batch = X_b_train.dot(theta) \n",
        "\n",
        "lin_batch_mse = mean_squared_error(y_reg_train, prediction_lin_batch)\n",
        "lin_batch_rmse = np.sqrt(lin_batch_mse)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "40bb109c-8269-4f6c-ae87-de37674cf289",
        "id": "rKKe5zgM24iS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##stochastic batch GD - fast\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#mmse\n",
        "grid_param = {'max_iter' : [1000],#,1500\n",
        "              'tol':[1e-3],\n",
        "              'penalty':['l1'], #None,'l2','l1','elasticnet',\n",
        "              'eta0':[0.01] # ,0.05,0.1,0.2\n",
        "              }\n",
        "sgd_mmse_reg = SGDRegressor(random_state=42)\n",
        "sgd_mmse_reg = GridSearchCV(sgd_mmse_reg,grid_param,cv=5,\n",
        "                           scoring = 'neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "sgd_mmse_reg.fit(X_train, y_mmse_train.values.ravel())\n",
        "\n",
        "sgd_mmse_predictions = sgd_mmse_reg.predict(X_train)\n",
        "mmse_sgd_mse = mean_squared_error(y_mmse_train, sgd_mmse_predictions)\n",
        "mmse_sgd_rmse = np.sqrt(mmse_sgd_mse)\n",
        "\n",
        "#ada\n",
        "grid_param = {'max_iter' : [1000],#,1500\n",
        "              'tol':[1e-3],\n",
        "              'penalty':['l2'], #None,'l2','l1','elasticnet',\n",
        "              'eta0':[0.01], # ,0.05,0.1,0.2\n",
        "              }\n",
        "sgd_ada_reg = SGDRegressor(random_state=42)\n",
        "sgd_ada_reg = GridSearchCV(sgd_ada_reg,grid_param,cv=5,\n",
        "                           scoring = 'neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "sgd_ada_reg.fit(X_train, y_ada_train.values.ravel())\n",
        "\n",
        "sgd_ada_predictions = sgd_ada_reg.predict(X_train)\n",
        "ada_sgd_mse = mean_squared_error(y_ada_train, sgd_ada_predictions)\n",
        "ada_sgd_rmse = np.sqrt(ada_sgd_mse)\n",
        "sgd_rmse = (ada_sgd_rmse + mmse_sgd_rmse)/2\n",
        "\n",
        "scores = cross_val_score(sgd_mmse_reg,X_train,y_mmse_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "sgd_mmse_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "scores = cross_val_score(sgd_ada_reg,X_train,y_ada_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "sgd_ada_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "sgd_scores = (sgd_mmse_scores+sgd_ada_scores)/2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "698096fc-465a-4b8a-be02-657b0d32553b",
        "id": "5DeNRI-E24iU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(\"batch GD rmse :\", lin_batch_rmse)\n",
        "print(\"SGD rmse: \", sgd_rmse)\n",
        "print(\"SGD validation loss : \",sgd_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch GD rmse : 4.878614932943138\n",
            "SGD rmse:  4.316445439063838\n",
            "SGD validation loss :  4.664304977955965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CHuzJs2e24iW",
        "colab": {}
      },
      "source": [
        "sgd_mmse_reg = sgd_mmse_reg.best_estimator_ #eta0 0.01, 1000, l1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O5tICrp624iX",
        "colab": {}
      },
      "source": [
        "sgd_ada_reg = sgd_ada_reg.best_estimator_ #l2 1000 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "943e58e6-2f7f-4f83-c35c-c48fcd7ca454",
        "id": "bIC4Z6cp24iZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "sgd_mmse_reg.fit(X_test,y_mmse_test)\n",
        "sgd_mmse_predictions = sgd_mmse_reg.predict(X_test)\n",
        "sgd_mmse_mse = mean_squared_error(y_mmse_test,sgd_mmse_predictions )\n",
        "sgd_mmse_rmse = np.sqrt(sgd_mmse_mse)\n",
        "\n",
        "sgd_ada_reg.fit(X_test,y_ada_test)\n",
        "sgd_ada_predictions = sgd_ada_reg.predict(X_test)\n",
        "sgd_ada_mse = mean_squared_error(y_ada_test,sgd_ada_predictions )\n",
        "sgd_ada_rmse = np.sqrt(sgd_ada_mse)\n",
        "\n",
        "sgd_test_rmse = (sgd_mmse_rmse + sgd_ada_rmse) /2\n",
        "print(\"sgd test rmse : \",sgd_test_rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sgd test rmse :  4.223729294191411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu-vDg2WILVU",
        "colab_type": "text"
      },
      "source": [
        "sgd를 사용하는게 일단 좋을 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDjgJ9kL24ic"
      },
      "source": [
        "**linear - regularization\n",
        "---\n",
        "grid search사용해서 best parameter로 한 best estimator로 loss계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Un0N88Z424id",
        "outputId": "3c1672b6-dd7a-421f-f238-48a560d5753c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "##1. ridge\n",
        "param_grid = {'alpha': [0,  0.1, 0.3,0.5,0.8,1],\n",
        "              'solver':[\"cholesky\"]}#,\"sag\"\n",
        "\n",
        "ridge_reg = Ridge(random_state=42)\n",
        "ridge_reg = GridSearchCV(ridge_reg, param_grid,cv=5,\n",
        "                         scoring = 'neg_mean_squared_error',\n",
        "                         return_train_score=True)\n",
        "ridge_reg.fit(X_train, y_reg_train)\n",
        "ridge_reg = ridge_reg.best_estimator_\n",
        "\n",
        "ridge_predictions = ridge_reg.predict(X_train)\n",
        "ridge_lin_mse = mean_squared_error(y_reg_train, ridge_predictions)\n",
        "ridge_lin_rmse = np.sqrt(ridge_lin_mse)\n",
        "ridge_lin_rmse\n",
        "\n",
        "scores = cross_val_score(ridge_reg,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "ridge_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "##2. lasso\n",
        "param_grid = {'alpha': [0,  0.1, 0.3,0.5,0.8, 1]}\n",
        "\n",
        "lasso_reg = Lasso(random_state=42)\n",
        "lasso_reg = GridSearchCV(ridge_reg, param_grid,cv=5,\n",
        "                         scoring = 'neg_mean_squared_error',\n",
        "                         return_train_score=True)\n",
        "lasso_reg.fit(X_train, y_reg_train.values)\n",
        "lasso_reg = lasso_reg.best_estimator_\n",
        "\n",
        "lasso_predictions = lasso_reg.predict(X_train)\n",
        "lasso_lin_mse = mean_squared_error(y_reg_train, lasso_predictions)\n",
        "lasso_lin_rmse = np.sqrt(lasso_lin_mse)\n",
        "\n",
        "scores = cross_val_score(lasso_reg,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "lasso_lin_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "##3. elasticnet\n",
        "param_grid = {'alpha': [0, 0.1, 0.3,0.5,0.8 ,1],\n",
        "              'l1_ratio':[ 0.1, 0.3, 0.5, 0.8]}\n",
        "\n",
        "ela_reg = ElasticNet(random_state = 42)\n",
        "ela_reg = GridSearchCV(ela_reg, param_grid,cv=5,\n",
        "                         scoring = 'neg_mean_squared_error',\n",
        "                         return_train_score=True)\n",
        "#반반씩!\n",
        "ela_reg.fit(X_train,y_reg_train)\n",
        "ela_reg = ela_reg.best_estimator_\n",
        "\n",
        "ela_predictions = ela_reg.predict(X_train)\n",
        "ela_lin_mse = mean_squared_error(y_reg_train, ela_predictions)\n",
        "ela_lin_rmse = np.sqrt(ela_lin_mse)\n",
        "\n",
        "scores = cross_val_score(ela_reg,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "ela_lin_scores = np.sqrt(-scores).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1707.165461841391, tolerance: 0.6079595183486236\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18561.551399893036, tolerance: 7.505639058704129\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1654.6505762627548, tolerance: 0.589966513761468\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18217.64114328085, tolerance: 7.498202394483944\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1694.479751816041, tolerance: 0.5940839449541286\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19263.017528840202, tolerance: 7.637204630229357\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1664.3330997292187, tolerance: 0.621812844036697\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18560.01446744154, tolerance: 7.606774999162843\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1680.7218340030836, tolerance: 0.6182852064220187\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18569.425609040987, tolerance: 7.613982222832568\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1707.165461841391, tolerance: 0.6079595183486236\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18561.551399893036, tolerance: 7.505639058704129\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1654.6505762627548, tolerance: 0.589966513761468\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18217.64114328085, tolerance: 7.498202394483944\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1694.479751816041, tolerance: 0.5940839449541286\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19263.017528840202, tolerance: 7.637204630229357\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1664.3330997292187, tolerance: 0.621812844036697\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18560.01446744154, tolerance: 7.606774999162843\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1680.7218340030836, tolerance: 0.6182852064220187\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18569.425609040987, tolerance: 7.613982222832568\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1707.165461841391, tolerance: 0.6079595183486236\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18561.551399893036, tolerance: 7.505639058704129\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1654.6505762627548, tolerance: 0.589966513761468\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18217.64114328085, tolerance: 7.498202394483944\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1694.479751816041, tolerance: 0.5940839449541286\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19263.017528840202, tolerance: 7.637204630229357\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1664.3330997292187, tolerance: 0.621812844036697\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18560.01446744154, tolerance: 7.606774999162843\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1680.7218340030836, tolerance: 0.6182852064220187\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18569.425609040987, tolerance: 7.613982222832568\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1707.165461841391, tolerance: 0.6079595183486236\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18561.551399893036, tolerance: 7.505639058704129\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1654.6505762627548, tolerance: 0.589966513761468\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18217.64114328085, tolerance: 7.498202394483944\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1694.479751816041, tolerance: 0.5940839449541286\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19263.017528840202, tolerance: 7.637204630229357\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1664.3330997292187, tolerance: 0.621812844036697\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18560.01446744154, tolerance: 7.606774999162843\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1680.7218340030836, tolerance: 0.6182852064220187\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18569.425609040987, tolerance: 7.613982222832568\n",
            "  positive)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9SwnTCDl24ie",
        "outputId": "d3e33ca9-508a-4098-d399-0f28f695550c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "print(\"1. ridge train loss: \",ridge_lin_rmse)\n",
        "print(\"ridge validation loss :\" , ridge_scores)\n",
        "\n",
        "print(\"\\n2. lasso train loss: \",lasso_lin_rmse)\n",
        "print(\"lasso validation loss :\" , lasso_lin_scores)\n",
        "\n",
        "print(\"\\n3. elasticnet train loss: \",ela_lin_rmse)\n",
        "print(\"elasticnet validation loss :\" , ela_lin_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. ridge train loss:  4.878654030626113\n",
            "ridge validation loss : 5.333708770391743\n",
            "\n",
            "2. lasso train loss:  4.878654030626113\n",
            "lasso validation loss : 5.333708770391743\n",
            "\n",
            "3. elasticnet train loss:  4.948324925573131\n",
            "elasticnet validation loss : 5.206069272959162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RbjPusBd24ig"
      },
      "source": [
        "randomforest를 optimization한거랑 sgd랑 비교할 것임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gWV896S24ig"
      },
      "source": [
        "**randomforest grid search\n",
        "---\n",
        "* grid search : cross validation과 hyperparameter튜닝을 동시에함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d9b0c24c-e40d-477a-ab66-5a4eaa77133c",
        "id": "CCkfDfm924ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    {'n_estimators': [1500],  #n_estimators : 생성할 tree개수-많을수록 좋은듯 500,1000,1500\n",
        "    'max_features': [35],    #max_features : 최대 선택할 특성의 수 34,35,36\n",
        "    'max_depth' : [15]\n",
        "     }\n",
        "  ]\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(rf_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(X_train,y_reg_train)\n",
        "\n",
        "print('best estimator : ')\n",
        "print(grid_search.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best estimator : \n",
            "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
            "                      max_features=35, max_leaf_nodes=None,\n",
            "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                      min_samples_leaf=1, min_samples_split=2,\n",
            "                      min_weight_fraction_leaf=0.0, n_estimators=1500,\n",
            "                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
            "                      warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3LXDM0up24ij",
        "outputId": "90494054-0334-4b9a-9711-9aab78ec5dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 15, 'max_features': 35, 'n_estimators': 1500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GokEnGQE24ik",
        "outputId": "0b013c4e-7a19-4646-bcd4-00a0a2e7e3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "final_reg = grid_search.best_estimator_\n",
        "\n",
        "final_reg.fit(X_train,y_reg_train)\n",
        "final_reg = MultiOutputRegressor(final_reg)\n",
        "final_reg.fit(X_train,y_reg_train)\n",
        "\n",
        "final_reg_predictions = final_reg.predict(X_train)\n",
        "final_reg_mse = mean_squared_error(y_reg_train, final_reg_predictions)\n",
        "final_reg_rmse = np.sqrt(final_reg_mse)\n",
        "\n",
        "scores = cross_val_score(final_reg,X_train,y_reg_train,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "final_reg_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "print(\"final regression model : randomforest\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"final model train loss : \", final_reg_rmse)\n",
        "print(\"final model validation loss: \",final_reg_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final regression model : randomforest\n",
            "--------------------------------------\n",
            "final model train loss :  2.0172615094186823\n",
            "final model validation loss:  5.206911213935146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ot7OUWoD24im"
      },
      "source": [
        "sgd regressor를 최종으로 적용하려고 했으나, test set에서 이상하게 잘 적용이 안되어서 그냥 randomforest를 사용했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Azqp5RwW24im"
      },
      "source": [
        "3) final regression model - whole X set으로 fit시킴\n",
        "---\n",
        "final : SGDRegressor로 한 linear model로 하려했으나, randomforest로 변경(real test set에 잘 맞지 않아서!!!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xyd9bb1_24in"
      },
      "source": [
        "final : SGD\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "703485ea-48b3-45b1-c98a-0d09b50ecf48",
        "id": "sTfsiah_24in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#mmse\n",
        "grid_param = {'max_iter' : [40],#50,100,800,1000,1500\n",
        "              'tol':[1e-3],\n",
        "              'penalty':['l1'], #None,'l2','l1','elasticnet',\n",
        "              'eta0':[0.01] # ,0.05,0.1,0.2\n",
        "              }\n",
        "sgd_mmse_reg = SGDRegressor(random_state=42)\n",
        "sgd_mmse_reg = GridSearchCV(sgd_mmse_reg,grid_param,cv=5,\n",
        "                           scoring = 'neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "sgd_mmse_reg.fit(X, y_mmse.values.ravel())\n",
        "sgd_mmse_reg = sgd_mmse_reg.best_estimator_\n",
        "sgd_mmse_reg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
              "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
              "             learning_rate='invscaling', loss='squared_loss', max_iter=40,\n",
              "             n_iter_no_change=5, penalty='l1', power_t=0.25, random_state=42,\n",
              "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
              "             warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0dc23c28-b25b-48e7-df9f-1bf1294eb000",
        "id": "Wd8lgS_Z24ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sgd_mmse_predictions = sgd_mmse_reg.predict(X)\n",
        "mmse_sgd_mse = mean_squared_error(y_mmse, sgd_mmse_predictions)\n",
        "mmse_sgd_rmse = np.sqrt(mmse_sgd_mse)\n",
        "mmse_sgd_rmse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0456345028998557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f81d1a5a-efee-4148-da24-8daae0249cf7",
        "id": "URBJpBr224is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#ada\n",
        "grid_param = {'max_iter' : [40],#,1000,1500\n",
        "              'tol':[1e-3],\n",
        "              'penalty':['l1'], #None,'l2','l1','elasticnet',\n",
        "              'eta0':[0.01], # ,0.05,0.1,0.2\n",
        "              }\n",
        "sgd_ada_reg = SGDRegressor(random_state=42)\n",
        "sgd_ada_reg = GridSearchCV(sgd_ada_reg,grid_param,cv=5,\n",
        "                           scoring = 'neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "sgd_ada_reg.fit(X, y_ada.values.ravel())\n",
        "sgd_ada_reg = sgd_ada_reg.best_estimator_\n",
        "sgd_ada_reg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
              "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
              "             learning_rate='invscaling', loss='squared_loss', max_iter=40,\n",
              "             n_iter_no_change=5, penalty='l1', power_t=0.25, random_state=42,\n",
              "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
              "             warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOrrCqdj24iv",
        "colab": {}
      },
      "source": [
        "sgd_ada_predictions = sgd_ada_reg.predict(X)\n",
        "ada_sgd_mse = mean_squared_error(y_ada, sgd_ada_predictions)\n",
        "ada_sgd_rmse = np.sqrt(ada_sgd_mse)\n",
        "sgd_rmse = (ada_sgd_rmse + mmse_sgd_rmse)/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XSf1Snf424iy",
        "colab": {}
      },
      "source": [
        "scores = cross_val_score(sgd_mmse_reg,X,y_mmse.values.ravel(),\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "sgd_mmse_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "scores = cross_val_score(sgd_ada_reg,X,y_ada.values.ravel(),\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "sgd_ada_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "sgd_scores = (sgd_mmse_scores+sgd_ada_scores)/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1aa8616e-8e9e-4415-d1a3-f4df4cb671ca",
        "id": "mY3h9vUe24i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('final model train loss : ', sgd_rmse)\n",
        "print('final model validation loss : ',sgd_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final model train loss :  4.414766701160454\n",
            "final model validation loss :  4.6716340112644845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9U3hfgo24i2",
        "colab": {}
      },
      "source": [
        "final_mmse_reg = sgd_mmse_reg#.fit(X,y_mmse.values.ravel())\n",
        "final_ada_reg = sgd_ada_reg#.fit(X,y_ada.values.ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VDWeZ6VM24i4"
      },
      "source": [
        "이게 좋을 것 같아서 썼었는데, 실제로 값이 굉장히 이상하게 나온다. \n",
        "\n",
        "주어진 train set에서는 잘 맞는데 이상하게 주어진 test set에 적용시키면 값의 범위를 벗어난다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MRrNZ9AP24i5"
      },
      "source": [
        "final : randomforest\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "953f5bf2-f577-45e6-a8d1-76d22a5b6449",
        "id": "tEQQAZ5Z24i5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    {'n_estimators': [1500],  #n_estimators : 생성할 tree개수-많을수록 좋은듯 500,1000,1500\n",
        "    'max_features': [34],    #max_features : 최대 선택할 특성의 수 34,35,36\n",
        "    'max_depth' : [15]\n",
        "     }\n",
        "  ]\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(rf_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(X,y_reg)\n",
        "\n",
        "print('best estimator : ')\n",
        "print(grid_search.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best estimator : \n",
            "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
            "                      max_features=34, max_leaf_nodes=None,\n",
            "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                      min_samples_leaf=1, min_samples_split=2,\n",
            "                      min_weight_fraction_leaf=0.0, n_estimators=1500,\n",
            "                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
            "                      warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9041af57-83dd-4371-a232-3f4909a7cb29",
        "id": "WsTjt_lP24i7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "final_reg = grid_search.best_estimator_\n",
        "\n",
        "final_reg.fit(X,y_reg)\n",
        "final_reg = MultiOutputRegressor(final_reg)\n",
        "final_reg.fit(X,y_reg)\n",
        "\n",
        "final_reg_predictions = final_reg.predict(X)\n",
        "final_reg_mse = mean_squared_error(y_reg, final_reg_predictions)\n",
        "final_reg_rmse = np.sqrt(final_reg_mse)\n",
        "\n",
        "scores = cross_val_score(final_reg,X,y_reg,\n",
        "                         scoring=\"neg_mean_squared_error\",cv=5)\n",
        "final_reg_scores = np.sqrt(-scores).mean()\n",
        "\n",
        "print(\"final regression model : randomforest\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"final model train loss : \", final_reg_rmse)\n",
        "print(\"final model validation loss: \",final_reg_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final regression model : randomforest\n",
            "--------------------------------------\n",
            "final model train loss :  2.0519076740841107\n",
            "final model validation loss:  5.187938017583692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9ENIH3XWxK",
        "colab_type": "text"
      },
      "source": [
        "# 2. Classification \n",
        "\n",
        "multiclass classification\n",
        "\n",
        "할 수 있는 모델 : \n",
        "* SVM(linear, poly,rbf,sigmoid)\n",
        "* OneVsRestClassifier\n",
        "* RandomForestClassifier\n",
        "* softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMcJomT4nQlt",
        "colab_type": "text"
      },
      "source": [
        "train,test나누고 train으로 일단 validation 다 해본 후 거기서 젤 작은거 고르고, 젤 마지막으로 test set에 대해서 train에서 한 모델들을 적용시켜서 제일 작은거 고른다.\n",
        "\n",
        "모두 다 할 필요는 없겠지만, 할 수 있는거는 각각의 모델마다 grid_search해서 최적의 parameter가진 model들끼리 비교하게 한다.\n",
        "\n",
        "둘다 정확도가 높은 모델을 선택한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBEck9dOn1XB",
        "colab_type": "code",
        "outputId": "e1f79699-e20c-4aaf-a8e5-c6f7ac1e10ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_cls = y_cls.values.T[0]\n",
        "y_cls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 1, ..., 3, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd1TWCa7Xbni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zctieFy1JLQ9",
        "colab_type": "text"
      },
      "source": [
        "regression한 값인 MMSE, ADAS13두 개를 classification할 때 쓴다.(X_plus)\n",
        "\n",
        "이거 안쓰면 최고 정확도가 60퍼가 넘지 않는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2t0Zq7Dn13E",
        "colab_type": "text"
      },
      "source": [
        "1) model selection\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmvthpYsXbmo",
        "colab_type": "code",
        "outputId": "1cb2de15-bea4-47d1-9172-eefcde82594d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#svm\n",
        "lin_svm_clf = LinearSVC(C=0.1, loss=\"hinge\", random_state=42).fit(X_plus_train,y_cls_train)\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=2)),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
        "    ])\n",
        "\n",
        "polynomial_svm_clf.fit(X_plus_train, y_cls_train)\n",
        "\n",
        "poly_svm_clf = SVC(kernel=\"poly\",degree = 2, random_state=42).fit(X_plus_train, y_cls_train)\n",
        "rbf_svm_clf = SVC(kernel=\"rbf\",gamma=\"auto\", random_state=42).fit(X_plus_train, y_cls_train)\n",
        "sig_svm_clf = SVC(kernel=\"sigmoid\",gamma=\"auto\", random_state=42).fit(X_plus_train, y_cls_train)\n",
        "\n",
        "lin_svm_val = cross_val_score(lin_svm_clf,X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "poly_feature_lin_svm_val=  cross_val_score(polynomial_svm_clf,X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "poly_svm_val = cross_val_score(poly_svm_clf,X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "rbf_svm_val = cross_val_score(rbf_svm_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "sig_svm_val = cross_val_score(sig_svm_clf,X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "#random forest\n",
        "forest_clf = RandomForestClassifier(n_estimators=1000, criterion='entropy', random_state=42).fit(X_plus_train,y_cls_train)\n",
        "\n",
        "forest_val = cross_val_score(forest_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "#ovr,ovc\n",
        "ovr_svm_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42)).fit(X_plus_train, y_cls_train)\n",
        "ovc_svm_clf = OneVsOneClassifier(SVC(gamma=\"auto\", random_state=42)).fit(X_plus_train, y_cls_train)\n",
        "ovr_rf_clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=100,random_state=42)).fit(X_plus_train, y_cls_train)\n",
        "ovc_rf_clf = OneVsOneClassifier(RandomForestClassifier(n_estimators=100,random_state=42)).fit(X_plus_train, y_cls_train)\n",
        "\n",
        "ovr_svm_val = cross_val_score(ovr_svm_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "ovc_svm_val = cross_val_score(ovc_svm_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "ovr_rf_val = cross_val_score(ovr_rf_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "ovc_rf_val = cross_val_score(ovc_rf_clf, X_plus_train, y_cls_train, cv=3, scoring=\"accuracy\").mean()\n",
        "\n",
        "#softmax\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",\n",
        "                                 solver=\"lbfgs\", C=0.1, random_state=42)\n",
        "#C값이 regularization 강도 결정\n",
        "# C 낮을수록 계수를 0으로 근사-regularization강화됨\n",
        "\n",
        "softmax_reg.fit(X_plus_train,y_cls_train)\n",
        "softmax_val = cross_val_score(softmax_reg,X_plus_train,y_cls_train, cv=3, scoring=\"accuracy\").mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:516: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwQHPc1IpGBK",
        "colab_type": "code",
        "outputId": "5b6fa019-8407-435f-d6d6-7ada0c21b8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "print(\"linear svm val : \",lin_svm_val)\n",
        "print(\"polynomial lin svm val\",poly_feature_lin_svm_val)\n",
        "print(\"poly svm val : \",poly_svm_val)\n",
        "print(\"rbf svm val\",rbf_svm_val)\n",
        "print(\"sigmoid svm val\",sig_svm_val)\n",
        "print(\"random val : \",forest_val)\n",
        "print(\"ovr svm val\",ovr_svm_val)\n",
        "print(\"ovc svm val\",ovc_svm_val)\n",
        "print(\"ovr rf val\",ovr_rf_val)\n",
        "print(\"ovc rf val\",ovc_rf_val)\n",
        "print(\"soft max val : \", softmax_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear svm val :  0.6376289191773941\n",
            "polynomial lin svm val 0.5715630326784631\n",
            "poly svm val :  0.5651400237001195\n",
            "rbf svm val 0.6917041033620436\n",
            "sigmoid svm val 0.5385022694834657\n",
            "random val :  0.6981421238523403\n",
            "ovr svm val 0.6697057472615979\n",
            "ovc svm val 0.6917041033620436\n",
            "ovr rf val 0.7119340686055287\n",
            "ovc rf val 0.7137454160159072\n",
            "soft max val :  0.7009575329191685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uiBcExhYCI5",
        "colab_type": "text"
      },
      "source": [
        "randomforest로 important feature찾고 중요한 feature들만 적용해서 하려했으나, 더 정확도가 떨어졌다. 그래서 그냥 important feature안 찾음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgZEcQZaXwa",
        "colab_type": "text"
      },
      "source": [
        "2) randomforest grid search\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOiKkKsuYTHt",
        "colab_type": "text"
      },
      "source": [
        "**randomforest선택"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqTN-E_DXbjZ",
        "colab_type": "code",
        "outputId": "706b4e49-6300-40a3-d29d-a0a83917d0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "param_grid = {\n",
        "    'max_features' : ['auto'], # 'sqrt','log2'\n",
        "    'max_depth' : [15], # 10,20,30,40,50,60,70, 14,16\n",
        "    'criterion' : ['entropy'], #'gini'\n",
        "    'min_samples_split' : [3], #2,10  4,5\n",
        "    'bootstrap' : [False] #True\n",
        "}\n",
        "\n",
        "GS_rfc = GridSearchCV(forest_clf,param_grid,\n",
        "                      cv=3, scoring = \"accuracy\")\n",
        "GS_rfc.fit(X_plus,y_cls)\n",
        "GS_rfc.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': False,\n",
              " 'criterion': 'entropy',\n",
              " 'max_depth': 15,\n",
              " 'max_features': 'auto',\n",
              " 'min_samples_split': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KJJ-QsLYbXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_clf = forest_clf = GS_rfc.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT67zus9Yc-h",
        "colab_type": "code",
        "outputId": "b509052c-4ccc-48a1-9934-dab5478b07c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "GS_rfc.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7241379310344828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzU39WGplESA",
        "colab_type": "text"
      },
      "source": [
        "**test data에 대해서 보기\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caYMkJqlLi5",
        "colab_type": "text"
      },
      "source": [
        "X_plus_test 안쓰고, X_test쓸것임. \n",
        "final regression적용시키고 볼 것임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHBzrTu4ve8t",
        "colab_type": "text"
      },
      "source": [
        "* 데이터 분포봄 : 71*71모든 쌍을 만들어도 거의 모두 이런식의 분포를 보인다.\n",
        "\n",
        "--> linear는 맞지는 않을 것같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMxSEwi3vgW_",
        "colab_type": "code",
        "outputId": "40bab3bd-cce1-4efc-f1b2-5d78ab7f328c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "'''\n",
        "for i in range(0,71):\n",
        "  for j in range(0,71):\n",
        "    if i==j: pass\n",
        "    else:\n",
        "      plt.plot((y_cls.values==1).T[0] * X[:,i],(y_cls.values==1).T[0] * X[:,j],\"bs\",label=\"class 1\")\n",
        "      plt.plot((y_cls.values==2).T[0] * X[:,i],(y_cls.values==2).T[0] * X[:,j],\"rs\",label=\"class 2\")\n",
        "      plt.plot((y_cls.values==3).T[0] * X[:,i],(y_cls.values==3).T[0] * X[:,j],\"gs\",label =\"class 3\")  \n",
        "      plt.show()\n",
        "'''\n",
        "plt.plot((y_cls==1)* X[:,1],(y_cls==1)* X[:,2],\"bs\",label=\"class 1\")\n",
        "plt.plot((y_cls==2) * X[:,1],(y_cls==2) * X[:,2],\"rs\",label=\"class 2\")\n",
        "plt.plot((y_cls==3) * X[:,1],(y_cls==3) * X[:,2],\"gs\",label =\"class 3\")  \n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V29GEne7Z8HS",
        "colab_type": "text"
      },
      "source": [
        "# test set(제출용)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94mxJbDPaAPg",
        "colab_type": "code",
        "outputId": "e3702a41-a981-4c1a-c93c-d733bde4f583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "# Load test dataset\n",
        "csv_file_test = '/content/gdrive/My Drive/BNCS401_Midterm_Project/Test_data_ageupdated.csv'  # Set your path\n",
        "test_data = pd.read_csv(csv_file_test)\n",
        "test_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RID</th>\n",
              "      <th>AGE</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002</td>\n",
              "      <td>76.3</td>\n",
              "      <td>1.996</td>\n",
              "      <td>2.220</td>\n",
              "      <td>2.342</td>\n",
              "      <td>2.803</td>\n",
              "      <td>2.230</td>\n",
              "      <td>1.402</td>\n",
              "      <td>1.764</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.026</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.964</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.454</td>\n",
              "      <td>1.925</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.311</td>\n",
              "      <td>3.016</td>\n",
              "      <td>2.317</td>\n",
              "      <td>1.121</td>\n",
              "      <td>2.956</td>\n",
              "      <td>2.936</td>\n",
              "      <td>2.219</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.320</td>\n",
              "      <td>1.573</td>\n",
              "      <td>3.061</td>\n",
              "      <td>2.696</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.283</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.097</td>\n",
              "      <td>2.686</td>\n",
              "      <td>1.768</td>\n",
              "      <td>2.387</td>\n",
              "      <td>2.674</td>\n",
              "      <td>1.819</td>\n",
              "      <td>2.135</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.003</td>\n",
              "      <td>2.374</td>\n",
              "      <td>1.411</td>\n",
              "      <td>1.727</td>\n",
              "      <td>2.434</td>\n",
              "      <td>2.157</td>\n",
              "      <td>2.063</td>\n",
              "      <td>3.182</td>\n",
              "      <td>2.085</td>\n",
              "      <td>2.485</td>\n",
              "      <td>1.905</td>\n",
              "      <td>2.488</td>\n",
              "      <td>2.182</td>\n",
              "      <td>3.563</td>\n",
              "      <td>2.187</td>\n",
              "      <td>0.975</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.253</td>\n",
              "      <td>2.476</td>\n",
              "      <td>1.639</td>\n",
              "      <td>3.655</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.461</td>\n",
              "      <td>2.157</td>\n",
              "      <td>2.911</td>\n",
              "      <td>2.410</td>\n",
              "      <td>1.968</td>\n",
              "      <td>2.800</td>\n",
              "      <td>1.767</td>\n",
              "      <td>2.080</td>\n",
              "      <td>2.766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>260</td>\n",
              "      <td>78.6</td>\n",
              "      <td>2.209</td>\n",
              "      <td>2.805</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.209</td>\n",
              "      <td>1.517</td>\n",
              "      <td>1.648</td>\n",
              "      <td>2.457</td>\n",
              "      <td>2.184</td>\n",
              "      <td>2.132</td>\n",
              "      <td>3.077</td>\n",
              "      <td>2.279</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.342</td>\n",
              "      <td>4.067</td>\n",
              "      <td>2.303</td>\n",
              "      <td>1.082</td>\n",
              "      <td>3.035</td>\n",
              "      <td>2.973</td>\n",
              "      <td>2.370</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.914</td>\n",
              "      <td>3.925</td>\n",
              "      <td>2.438</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.932</td>\n",
              "      <td>2.328</td>\n",
              "      <td>2.134</td>\n",
              "      <td>2.831</td>\n",
              "      <td>1.838</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.219</td>\n",
              "      <td>2.797</td>\n",
              "      <td>2.456</td>\n",
              "      <td>2.797</td>\n",
              "      <td>2.147</td>\n",
              "      <td>1.469</td>\n",
              "      <td>1.729</td>\n",
              "      <td>2.692</td>\n",
              "      <td>2.329</td>\n",
              "      <td>2.274</td>\n",
              "      <td>2.994</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.627</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.458</td>\n",
              "      <td>2.314</td>\n",
              "      <td>3.839</td>\n",
              "      <td>2.167</td>\n",
              "      <td>1.228</td>\n",
              "      <td>2.366</td>\n",
              "      <td>2.834</td>\n",
              "      <td>2.320</td>\n",
              "      <td>1.607</td>\n",
              "      <td>3.849</td>\n",
              "      <td>2.843</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.484</td>\n",
              "      <td>2.811</td>\n",
              "      <td>2.402</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.685</td>\n",
              "      <td>1.795</td>\n",
              "      <td>2.332</td>\n",
              "      <td>2.965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1406</td>\n",
              "      <td>61.1</td>\n",
              "      <td>2.114</td>\n",
              "      <td>3.157</td>\n",
              "      <td>2.700</td>\n",
              "      <td>2.856</td>\n",
              "      <td>2.463</td>\n",
              "      <td>1.610</td>\n",
              "      <td>1.671</td>\n",
              "      <td>2.720</td>\n",
              "      <td>2.174</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.353</td>\n",
              "      <td>2.818</td>\n",
              "      <td>1.856</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.372</td>\n",
              "      <td>3.734</td>\n",
              "      <td>2.217</td>\n",
              "      <td>1.286</td>\n",
              "      <td>3.094</td>\n",
              "      <td>3.091</td>\n",
              "      <td>2.362</td>\n",
              "      <td>2.674</td>\n",
              "      <td>2.513</td>\n",
              "      <td>1.653</td>\n",
              "      <td>3.764</td>\n",
              "      <td>2.984</td>\n",
              "      <td>2.735</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.823</td>\n",
              "      <td>2.546</td>\n",
              "      <td>1.992</td>\n",
              "      <td>2.768</td>\n",
              "      <td>1.905</td>\n",
              "      <td>2.517</td>\n",
              "      <td>2.869</td>\n",
              "      <td>2.010</td>\n",
              "      <td>3.147</td>\n",
              "      <td>2.558</td>\n",
              "      <td>2.763</td>\n",
              "      <td>2.175</td>\n",
              "      <td>1.532</td>\n",
              "      <td>1.609</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.088</td>\n",
              "      <td>2.174</td>\n",
              "      <td>3.211</td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.840</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.815</td>\n",
              "      <td>2.254</td>\n",
              "      <td>3.713</td>\n",
              "      <td>2.452</td>\n",
              "      <td>1.164</td>\n",
              "      <td>2.778</td>\n",
              "      <td>2.551</td>\n",
              "      <td>2.402</td>\n",
              "      <td>1.810</td>\n",
              "      <td>3.646</td>\n",
              "      <td>3.352</td>\n",
              "      <td>2.766</td>\n",
              "      <td>2.249</td>\n",
              "      <td>2.990</td>\n",
              "      <td>2.401</td>\n",
              "      <td>1.983</td>\n",
              "      <td>2.666</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.926</td>\n",
              "      <td>2.903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>516</td>\n",
              "      <td>87.6</td>\n",
              "      <td>1.961</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.466</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.494</td>\n",
              "      <td>1.317</td>\n",
              "      <td>1.613</td>\n",
              "      <td>2.371</td>\n",
              "      <td>2.045</td>\n",
              "      <td>2.083</td>\n",
              "      <td>2.429</td>\n",
              "      <td>2.243</td>\n",
              "      <td>2.563</td>\n",
              "      <td>1.940</td>\n",
              "      <td>2.612</td>\n",
              "      <td>2.379</td>\n",
              "      <td>4.150</td>\n",
              "      <td>2.147</td>\n",
              "      <td>1.147</td>\n",
              "      <td>2.976</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.079</td>\n",
              "      <td>3.115</td>\n",
              "      <td>2.432</td>\n",
              "      <td>1.599</td>\n",
              "      <td>3.674</td>\n",
              "      <td>1.981</td>\n",
              "      <td>2.547</td>\n",
              "      <td>2.228</td>\n",
              "      <td>2.755</td>\n",
              "      <td>2.501</td>\n",
              "      <td>2.031</td>\n",
              "      <td>2.635</td>\n",
              "      <td>1.751</td>\n",
              "      <td>2.175</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.430</td>\n",
              "      <td>2.514</td>\n",
              "      <td>2.485</td>\n",
              "      <td>2.659</td>\n",
              "      <td>2.382</td>\n",
              "      <td>1.270</td>\n",
              "      <td>1.718</td>\n",
              "      <td>2.125</td>\n",
              "      <td>2.048</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.853</td>\n",
              "      <td>2.295</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.049</td>\n",
              "      <td>2.504</td>\n",
              "      <td>2.457</td>\n",
              "      <td>4.151</td>\n",
              "      <td>2.374</td>\n",
              "      <td>1.062</td>\n",
              "      <td>2.409</td>\n",
              "      <td>2.051</td>\n",
              "      <td>2.259</td>\n",
              "      <td>1.478</td>\n",
              "      <td>3.491</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.567</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.830</td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.032</td>\n",
              "      <td>2.497</td>\n",
              "      <td>1.730</td>\n",
              "      <td>2.279</td>\n",
              "      <td>2.825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4138</td>\n",
              "      <td>88.4</td>\n",
              "      <td>2.246</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.850</td>\n",
              "      <td>2.566</td>\n",
              "      <td>1.770</td>\n",
              "      <td>1.921</td>\n",
              "      <td>2.398</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.200</td>\n",
              "      <td>2.588</td>\n",
              "      <td>2.397</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.038</td>\n",
              "      <td>2.323</td>\n",
              "      <td>2.396</td>\n",
              "      <td>3.350</td>\n",
              "      <td>2.415</td>\n",
              "      <td></td>\n",
              "      <td>3.084</td>\n",
              "      <td>3.030</td>\n",
              "      <td>2.306</td>\n",
              "      <td>2.816</td>\n",
              "      <td>2.366</td>\n",
              "      <td>1.930</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.292</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.355</td>\n",
              "      <td>2.147</td>\n",
              "      <td>2.626</td>\n",
              "      <td>1.933</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.710</td>\n",
              "      <td>2.350</td>\n",
              "      <td>2.913</td>\n",
              "      <td>2.251</td>\n",
              "      <td>2.799</td>\n",
              "      <td>2.463</td>\n",
              "      <td>1.671</td>\n",
              "      <td>1.967</td>\n",
              "      <td>2.403</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.358</td>\n",
              "      <td>3.378</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.607</td>\n",
              "      <td>2.122</td>\n",
              "      <td>2.358</td>\n",
              "      <td>2.253</td>\n",
              "      <td>3.548</td>\n",
              "      <td>2.239</td>\n",
              "      <td></td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.665</td>\n",
              "      <td>2.497</td>\n",
              "      <td>1.858</td>\n",
              "      <td>2.393</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.452</td>\n",
              "      <td>2.327</td>\n",
              "      <td>2.644</td>\n",
              "      <td>1.992</td>\n",
              "      <td>2.141</td>\n",
              "      <td>2.895</td>\n",
              "      <td>1.838</td>\n",
              "      <td>2.533</td>\n",
              "      <td>2.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>1090</td>\n",
              "      <td>71.3</td>\n",
              "      <td>1.918</td>\n",
              "      <td>2.442</td>\n",
              "      <td>2.224</td>\n",
              "      <td>2.555</td>\n",
              "      <td>2.068</td>\n",
              "      <td>1.455</td>\n",
              "      <td>1.771</td>\n",
              "      <td>2.435</td>\n",
              "      <td>2.062</td>\n",
              "      <td>2.075</td>\n",
              "      <td>3.014</td>\n",
              "      <td>2.208</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.526</td>\n",
              "      <td>2.051</td>\n",
              "      <td>3.770</td>\n",
              "      <td>2.354</td>\n",
              "      <td>1.024</td>\n",
              "      <td>2.962</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.239</td>\n",
              "      <td>2.301</td>\n",
              "      <td>2.186</td>\n",
              "      <td>1.810</td>\n",
              "      <td>3.251</td>\n",
              "      <td>2.831</td>\n",
              "      <td>2.518</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.783</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.107</td>\n",
              "      <td>2.486</td>\n",
              "      <td>1.750</td>\n",
              "      <td>2.213</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.001</td>\n",
              "      <td>2.804</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.447</td>\n",
              "      <td>2.342</td>\n",
              "      <td>1.355</td>\n",
              "      <td>1.825</td>\n",
              "      <td>2.621</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.206</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.490</td>\n",
              "      <td>1.984</td>\n",
              "      <td>2.619</td>\n",
              "      <td>2.222</td>\n",
              "      <td>3.873</td>\n",
              "      <td>2.563</td>\n",
              "      <td>1.121</td>\n",
              "      <td>2.114</td>\n",
              "      <td>2.445</td>\n",
              "      <td>2.393</td>\n",
              "      <td>1.757</td>\n",
              "      <td>3.429</td>\n",
              "      <td>2.494</td>\n",
              "      <td>2.697</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.768</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.224</td>\n",
              "      <td>2.534</td>\n",
              "      <td>1.975</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>669</td>\n",
              "      <td>63.6</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.511</td>\n",
              "      <td>2.524</td>\n",
              "      <td>1.377</td>\n",
              "      <td>1.759</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.091</td>\n",
              "      <td>2.039</td>\n",
              "      <td>2.957</td>\n",
              "      <td>2.117</td>\n",
              "      <td>2.411</td>\n",
              "      <td>2.144</td>\n",
              "      <td>2.477</td>\n",
              "      <td>2.414</td>\n",
              "      <td>3.278</td>\n",
              "      <td>2.404</td>\n",
              "      <td>1.212</td>\n",
              "      <td>2.880</td>\n",
              "      <td>2.832</td>\n",
              "      <td>2.404</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.241</td>\n",
              "      <td>1.813</td>\n",
              "      <td>3.050</td>\n",
              "      <td>2.773</td>\n",
              "      <td>2.593</td>\n",
              "      <td>2.338</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.295</td>\n",
              "      <td>2.139</td>\n",
              "      <td>2.440</td>\n",
              "      <td>1.753</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.833</td>\n",
              "      <td>2.042</td>\n",
              "      <td>2.673</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.551</td>\n",
              "      <td>2.293</td>\n",
              "      <td>1.380</td>\n",
              "      <td>1.695</td>\n",
              "      <td>2.481</td>\n",
              "      <td>1.954</td>\n",
              "      <td>2.283</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.165</td>\n",
              "      <td>2.385</td>\n",
              "      <td>1.941</td>\n",
              "      <td>2.491</td>\n",
              "      <td>2.223</td>\n",
              "      <td>3.438</td>\n",
              "      <td>1.753</td>\n",
              "      <td>1.14</td>\n",
              "      <td>2.348</td>\n",
              "      <td>2.402</td>\n",
              "      <td>2.423</td>\n",
              "      <td>1.636</td>\n",
              "      <td>3.465</td>\n",
              "      <td>2.214</td>\n",
              "      <td>2.244</td>\n",
              "      <td>2.476</td>\n",
              "      <td>2.579</td>\n",
              "      <td>2.317</td>\n",
              "      <td>2.028</td>\n",
              "      <td>2.492</td>\n",
              "      <td>1.702</td>\n",
              "      <td>2.026</td>\n",
              "      <td>2.627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>298</td>\n",
              "      <td>76.4</td>\n",
              "      <td>2.075</td>\n",
              "      <td>2.326</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.815</td>\n",
              "      <td>2.251</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.683</td>\n",
              "      <td>2.356</td>\n",
              "      <td>2.330</td>\n",
              "      <td>2.151</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.642</td>\n",
              "      <td>2.100</td>\n",
              "      <td>2.776</td>\n",
              "      <td>2.503</td>\n",
              "      <td>3.833</td>\n",
              "      <td>2.586</td>\n",
              "      <td>1.197</td>\n",
              "      <td>2.903</td>\n",
              "      <td>2.775</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.420</td>\n",
              "      <td>2.425</td>\n",
              "      <td>1.830</td>\n",
              "      <td>3.475</td>\n",
              "      <td>2.861</td>\n",
              "      <td>2.727</td>\n",
              "      <td>2.437</td>\n",
              "      <td>2.688</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.645</td>\n",
              "      <td>2.268</td>\n",
              "      <td>2.761</td>\n",
              "      <td>2.073</td>\n",
              "      <td>2.741</td>\n",
              "      <td>2.430</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.093</td>\n",
              "      <td>1.326</td>\n",
              "      <td>1.910</td>\n",
              "      <td>2.308</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.163</td>\n",
              "      <td>3.024</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.626</td>\n",
              "      <td>2.169</td>\n",
              "      <td>2.523</td>\n",
              "      <td>2.523</td>\n",
              "      <td>3.883</td>\n",
              "      <td>2.259</td>\n",
              "      <td>1.247</td>\n",
              "      <td>2.341</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.450</td>\n",
              "      <td>1.601</td>\n",
              "      <td>4.016</td>\n",
              "      <td>2.652</td>\n",
              "      <td>2.542</td>\n",
              "      <td>2.367</td>\n",
              "      <td>2.747</td>\n",
              "      <td>2.293</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.636</td>\n",
              "      <td>1.732</td>\n",
              "      <td>2.416</td>\n",
              "      <td>2.827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>890</td>\n",
              "      <td>59.7</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.912</td>\n",
              "      <td>2.478</td>\n",
              "      <td>1.541</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.443</td>\n",
              "      <td>2.442</td>\n",
              "      <td>2.222</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.287</td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.125</td>\n",
              "      <td>2.727</td>\n",
              "      <td>2.314</td>\n",
              "      <td>3.879</td>\n",
              "      <td>2.699</td>\n",
              "      <td>1.087</td>\n",
              "      <td>2.876</td>\n",
              "      <td>2.833</td>\n",
              "      <td>1.952</td>\n",
              "      <td>2.601</td>\n",
              "      <td>2.544</td>\n",
              "      <td>1.796</td>\n",
              "      <td>3.235</td>\n",
              "      <td>2.810</td>\n",
              "      <td>2.884</td>\n",
              "      <td>2.210</td>\n",
              "      <td>2.788</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.172</td>\n",
              "      <td>2.634</td>\n",
              "      <td>1.860</td>\n",
              "      <td>2.475</td>\n",
              "      <td>2.550</td>\n",
              "      <td>2.264</td>\n",
              "      <td>3.112</td>\n",
              "      <td>2.547</td>\n",
              "      <td>2.667</td>\n",
              "      <td>2.316</td>\n",
              "      <td>1.534</td>\n",
              "      <td>1.814</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.510</td>\n",
              "      <td>2.141</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.686</td>\n",
              "      <td>2.119</td>\n",
              "      <td>2.773</td>\n",
              "      <td>2.316</td>\n",
              "      <td>3.747</td>\n",
              "      <td>2.884</td>\n",
              "      <td>1.092</td>\n",
              "      <td>2.364</td>\n",
              "      <td>2.541</td>\n",
              "      <td>2.576</td>\n",
              "      <td>1.701</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.752</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.303</td>\n",
              "      <td>2.069</td>\n",
              "      <td>2.902</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.306</td>\n",
              "      <td>2.782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>1321</td>\n",
              "      <td>83.2</td>\n",
              "      <td>2.284</td>\n",
              "      <td>2.613</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.186</td>\n",
              "      <td>1.356</td>\n",
              "      <td>2.017</td>\n",
              "      <td>2.316</td>\n",
              "      <td>2.366</td>\n",
              "      <td>2.170</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.212</td>\n",
              "      <td>2.569</td>\n",
              "      <td>2.129</td>\n",
              "      <td>2.617</td>\n",
              "      <td>2.524</td>\n",
              "      <td>3.621</td>\n",
              "      <td>2.262</td>\n",
              "      <td>1.097</td>\n",
              "      <td>3.078</td>\n",
              "      <td>3.040</td>\n",
              "      <td>2.492</td>\n",
              "      <td>2.838</td>\n",
              "      <td>2.596</td>\n",
              "      <td>1.648</td>\n",
              "      <td>3.036</td>\n",
              "      <td>3.021</td>\n",
              "      <td>2.472</td>\n",
              "      <td>2.347</td>\n",
              "      <td>2.704</td>\n",
              "      <td>2.651</td>\n",
              "      <td>2.078</td>\n",
              "      <td>2.683</td>\n",
              "      <td>1.779</td>\n",
              "      <td>2.388</td>\n",
              "      <td>2.768</td>\n",
              "      <td>2.173</td>\n",
              "      <td>2.519</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.557</td>\n",
              "      <td>2.287</td>\n",
              "      <td>1.283</td>\n",
              "      <td>1.999</td>\n",
              "      <td>2.719</td>\n",
              "      <td>2.417</td>\n",
              "      <td>2.205</td>\n",
              "      <td>2.738</td>\n",
              "      <td>2.221</td>\n",
              "      <td>2.622</td>\n",
              "      <td>2.129</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.443</td>\n",
              "      <td>3.084</td>\n",
              "      <td>2.400</td>\n",
              "      <td>0.971</td>\n",
              "      <td>2.358</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.704</td>\n",
              "      <td>1.691</td>\n",
              "      <td>3.388</td>\n",
              "      <td>2.716</td>\n",
              "      <td>2.526</td>\n",
              "      <td>2.383</td>\n",
              "      <td>2.829</td>\n",
              "      <td>2.473</td>\n",
              "      <td>2.136</td>\n",
              "      <td>2.572</td>\n",
              "      <td>1.758</td>\n",
              "      <td>2.164</td>\n",
              "      <td>2.832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows × 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RID  ...  ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\n",
              "0    1002  ...                                       2.766\n",
              "1     260  ...                                       2.965\n",
              "2    1406  ...                                       2.903\n",
              "3     516  ...                                       2.825\n",
              "4    4138  ...                                       2.777\n",
              "..    ...  ...                                         ...\n",
              "339  1090  ...                                       2.622\n",
              "340   669  ...                                       2.627\n",
              "341   298  ...                                       2.827\n",
              "342   890  ...                                       2.782\n",
              "343  1321  ...                                       2.832\n",
              "\n",
              "[344 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeSeu0wagrNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test_data.drop(\"RID\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplIzRpgvILq",
        "colab_type": "code",
        "outputId": "50dd69c3-684a-4767-8e9a-c7eccb202ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "incomplete_rows = test_data[test_data.isnull().any(axis=1)]#.head()\n",
        "incomplete_rows"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [AGE, ST102TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST103TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST104TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST105TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST106TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST107TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST108TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST109TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST110TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST111TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST113TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST114TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST115TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST116TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST117TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST118TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST119TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST121TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST129TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST130TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST13TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST14TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST15TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST23TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST24TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST25TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST26TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST31TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST32TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST34TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST36TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST38TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST39TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST43TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST44TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST45TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST46TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST47TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST48TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST49TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST50TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST51TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST52TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST54TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST55TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST56TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST57TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST58TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST59TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST60TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST62TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST72TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST73TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST74TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST82TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST83TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST84TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST85TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST90TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST91TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST93TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST94TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST95TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST97TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16, ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hWRNfWovYJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def object_to_float(o): #o = train_data[\"~~\"] 형태\n",
        "  for i in range(343):\n",
        "    if o[i] == ' ':\n",
        "      o[i] = np.nan #nan으로 채움"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwZwfDOGvd0e",
        "colab_type": "code",
        "outputId": "05632038-95a4-457d-c210-3f4ba32370c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "object_to_float(test_data[\"ST64TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-WlqfbnvgCF",
        "colab_type": "code",
        "outputId": "55a8b6d7-f0e3-497b-9da2-f5c57d560138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "object_to_float(test_data[\"ST123TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFSsRxi4uoy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_ = data_pipeline.fit_transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAayXQqxOa2",
        "colab_type": "code",
        "outputId": "98bd460d-33a4-48d9-cd09-3d7dc7b0ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(344, 71)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuxWvWb4giqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "reg_mmse_predictions = final_mmse_reg.predict(X_)\n",
        "reg_ada_predictions = final_ada_reg.predict(X_)\n",
        "'''\n",
        "reg_predictions = final_reg.predict(X_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4fd01f0-bd8d-47bf-f12a-270cb3104cb0",
        "id": "u9HAH5LH0ioD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "prediction_reg = DataFrame({\"MMSE\":reg_predictions[:,0],\n",
        "                            \"ADA13\":reg_predictions[:,1]})\n",
        "'''\n",
        "prediction_reg = DataFrame({\"MMSE\":reg_mmse_predictions,\n",
        "                            \"ADA13\":reg_ada_predictions})\n",
        "                            '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprediction_reg = DataFrame({\"MMSE\":reg_mmse_predictions,\\n                            \"ADA13\":reg_ada_predictions})\\n                            '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4ca0cf3-673b-4a1b-ad69-21cdf94667ac",
        "id": "TZvzlFt-0ioE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "prediction_reg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ADA13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27.766236</td>\n",
              "      <td>13.256609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.256737</td>\n",
              "      <td>11.822731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.258680</td>\n",
              "      <td>12.356931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28.076554</td>\n",
              "      <td>13.567189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.305433</td>\n",
              "      <td>23.449884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>28.232073</td>\n",
              "      <td>13.908885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>28.100366</td>\n",
              "      <td>13.892758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>27.737613</td>\n",
              "      <td>11.930887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>27.149634</td>\n",
              "      <td>18.648599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>28.336669</td>\n",
              "      <td>12.903392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          MMSE      ADA13\n",
              "0    27.766236  13.256609\n",
              "1    28.256737  11.822731\n",
              "2    28.258680  12.356931\n",
              "3    28.076554  13.567189\n",
              "4    25.305433  23.449884\n",
              "..         ...        ...\n",
              "339  28.232073  13.908885\n",
              "340  28.100366  13.892758\n",
              "341  27.737613  11.930887\n",
              "342  27.149634  18.648599\n",
              "343  28.336669  12.903392\n",
              "\n",
              "[344 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_XUO7QQouS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_ = pd.concat([test_data, prediction_reg],axis=1)\n",
        "\n",
        "X_clf = data_pipeline.fit_transform(X_)\n",
        "\n",
        "clf_predictions = final_clf.predict(X_clf)\n",
        "\n",
        "prediction_clf = DataFrame({\"MMSE\":reg_predictions[:,0],\n",
        "                             \"ADA13\":reg_predictions[:,1],\n",
        "                            \"DXCHANGE\":clf_predictions})\n",
        "\n",
        "prediction = prediction_clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1joy-w1IxmMK",
        "colab_type": "code",
        "outputId": "568cd97a-dab9-4b02-92fc-983da376a540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MMSE</th>\n",
              "      <th>ADA13</th>\n",
              "      <th>DXCHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27.766236</td>\n",
              "      <td>13.256609</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.256737</td>\n",
              "      <td>11.822731</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.258680</td>\n",
              "      <td>12.356931</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28.076554</td>\n",
              "      <td>13.567189</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.305433</td>\n",
              "      <td>23.449884</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>28.232073</td>\n",
              "      <td>13.908885</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>28.100366</td>\n",
              "      <td>13.892758</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>27.737613</td>\n",
              "      <td>11.930887</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>27.149634</td>\n",
              "      <td>18.648599</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>28.336669</td>\n",
              "      <td>12.903392</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          MMSE      ADA13  DXCHANGE\n",
              "0    27.766236  13.256609         2\n",
              "1    28.256737  11.822731         2\n",
              "2    28.258680  12.356931         2\n",
              "3    28.076554  13.567189         2\n",
              "4    25.305433  23.449884         1\n",
              "..         ...        ...       ...\n",
              "339  28.232073  13.908885         2\n",
              "340  28.100366  13.892758         2\n",
              "341  27.737613  11.930887         1\n",
              "342  27.149634  18.648599         2\n",
              "343  28.336669  12.903392         2\n",
              "\n",
              "[344 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVWnBgZRkfPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction.to_csv('/content/gdrive/My Drive/Colab Notebooks/prediction.csv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}